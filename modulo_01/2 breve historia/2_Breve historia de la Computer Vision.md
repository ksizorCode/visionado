# Breve historia de la visión por ordenador

## Orígenes (1950s-1960s)
Los fundamentos se establecieron con los primeros trabajos en procesamiento de imágenes digitales. En 1957, Russell Kirsch creó la primera imagen digital escaneada (una foto de su hijo de 176x176 píxeles). Durante los años 60, Larry Roberts en MIT desarrolló técnicas para extraer información 3D de imágenes 2D, sentando las bases teóricas del campo.

## Primeras aproximaciones (1970s)
David Marr en MIT propuso su influyente teoría computacional de la visión, estableciendo un marco conceptual que perdura. Paralelamente, se desarrollaron los primeros algoritmos para detección de bordes y reconocimiento de patrones básicos. En informática general, aparecieron los primeros microprocesadores (Intel 4004 en 1971).

## Expansión algorítmica (1980s)
Se desarrollaron técnicas fundamentales como la transformada de Hough para detección de líneas y formas geométricas, y algoritmos de correspondencia estéreo para visión 3D. En el contexto más amplio de IA, nacieron las redes neuronales multicapa con el algoritmo de retropropagación (1986), aunque su aplicación masiva a visión llegaría décadas después.

## Consolidación (1990s)
Surgieron las primeras aplicaciones comerciales exitosas, especialmente en reconocimiento óptico de caracteres (OCR) y sistemas de inspección industrial. Se establecieron bases de datos estándar para evaluación de algoritmos. En informática, la World Wide Web (1991) democratizó el acceso a imágenes digitales.

## Revolución del aprendizaje profundo (2000s-2010s)
El momento decisivo llegó en 2012 con AlexNet, una red neuronal convolucional que revolucionó el reconocimiento de imágenes en el concurso ImageNet. Este hito marcó el inicio de la era del deep learning en visión por ordenador. Otros avances cruciales incluyeron las arquitecturas ResNet (2015) y la introducción de las redes generativas adversariales (GANs) en 2014.


## Era moderna (2010s-presente)
La visión por ordenador alcanzó y superó capacidades humanas en tareas específicas como clasificación de imágenes. Se desarrollaron aplicaciones masivas en reconocimiento facial, vehículos autónomos, realidad aumentada y diagnóstico médico. En IA general, surgieron los modelos transformer (2017) y posteriormente los modelos de lenguaje de gran escala como GPT, que comenzaron a integrarse con capacidades visuales.

## Convergencia actual
Hoy presenciamos la convergencia de visión por ordenador con modelos multimodales que combinan texto, imágenes y otros tipos de datos, representando la siguiente frontera en inteligencia artificial general.